{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvweuLArpisE"
      },
      "source": [
        "# Przetwarzanie języka naturalnego w spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instalacje"
      ],
      "metadata": {
        "id": "yFFxuBCR_ast"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy --upgrade"
      ],
      "metadata": {
        "id": "TB0ESe8Qr33y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Musimy wybrać model językowy https://spacy.io/usage/models"
      ],
      "metadata": {
        "id": "5RqVoqLT5He9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download pl_core_news_md"
      ],
      "metadata": {
        "id": "2bq22QjWribV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvkDGE3lpisR"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"pl_core_news_md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAhz4DxWpisT"
      },
      "source": [
        "Zmienna `nlp` zawiera średni model `pl_core_news_md` dla języka polskiego.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Operacja na tekście\n",
        "\n",
        "\n",
        "Przepuśćmy mały „dokument” przez parser języka naturalnego:"
      ],
      "metadata": {
        "id": "iFELu2mS_hH3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvlckO2SpisU"
      },
      "outputs": [],
      "source": [
        "text = \"Litwo, Ojczyzno moja, Ty jesteś jak zdrowie, ile Cię trzeba cenić ten tylko się dowie, kto Cię stracił.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDjO10gUpisZ"
      },
      "source": [
        "Najpierw stworzyliśmy z tekstu [doc](https://:spacy.io/api/doc), który jest kontenerem na dokument i wszystkie jego adnotacje. Następnie przejrzeliśmy dokument, aby zobaczyć, co przeanalizował _spaCy_.\n",
        "\n",
        "\n",
        "W tym prostym przypadku cały dokument to tylko jedno krótkie zdanie.\n",
        "Dla każdego słowa w tym zdaniu _spaCy_ utworzył [token](https://spacy.io/api/token) i uzyskaliśmy dostęp do pól w każdym tokenie, aby pokazać:\n",
        "\n",
        "  - surowy tekst\n",
        "  - [lemma](https://en.wikipedia.org/wiki/Lemma_(morphology)) – rdzeń słowa\n",
        "  - [część mowy](https://en.wikipedia.org/wiki/Część_mowy)\n",
        "  - flaga określająca, czy słowo jest _stopword_ – czyli popularnym słowem, które można odfiltrować"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEvnJFBMpisV"
      },
      "source": [
        "\n",
        "Przeformatujmy parsowanie _spaCy_ tego zdania jako ramkę danych [pandas](https://pandas.pydata.org/):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZB3BUdv1pisX"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "cols = (\"text\", \"lemma\", \"POS\", \"explain\", \"stopword\")\n",
        "rows = []\n",
        "\n",
        "for t in doc:\n",
        "    row = [t.text, t.lemma_, t.pos_, spacy.explain(t.pos_), t.is_stop]\n",
        "    rows.append(row)\n",
        "\n",
        "df = pd.DataFrame(rows, columns=cols)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas to taki \"pythonowy excel\", kiedy użyjemy go do danych mamy pod ręką móstwo opcji. Możemy na przykład zrobić łatwo wykres jak często występują dane części mowy:"
      ],
      "metadata": {
        "id": "ZwC8uAZkAN3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.POS.value_counts().plot(kind='bar')\n"
      ],
      "metadata": {
        "id": "1ZPBICyHAM0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Możemy dostać się do konkretnego słowa, jak jak w liście - poprzez podanie indeksu."
      ],
      "metadata": {
        "id": "-rbg1zyEBiAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc[21].lemma_"
      ],
      "metadata": {
        "id": "p7SDBegunqTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Im2iP5nNpisa"
      },
      "source": [
        "Następnie użyjemy biblioteki [displaCy](https://ines.io/blog/developing-displacy) do wizualizacji drzewa analizy dla tego zdania:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVfXisYupisb"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy\n",
        "\n",
        "displacy.render(doc, style=\"dep\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLhGVwTQpisc"
      },
      "source": [
        "W dłuższych dokumentach musimy dzielić na zdania - potrzebujemy wykrywania granicy zdań — znanej również jako _segmentacja zdań_ — oparte na wbudowanym/domyślnym [sentencizer](https://spacy.io/api/sentencizer):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F29YWvq5pisd"
      },
      "outputs": [],
      "source": [
        "text = \"W mieścinie pewnej, prowincji Manchy, której nazwiska nie powiem, żył niedawnymi czasy hidalgo pewien, z liczby tych, co to prócz spisy u siodła, szabliska starego, szkapy chudziny i paru gończych, niewiele co więcej mają. Rosolina powszednia, z baraniny częściej niż z wołowiny wygotowana na obiad, bigosik z resztek obiadu prawie co wieczór na kolację, co piątek soczewica, co sobota jaja sadzone po hiszpańsku, a na niedzielę gołąbeczek jakiś w dodatku do codziennej strawy, zjadały mu corocznie trzy czwarte części całego dochodu. Reszta szła na przyodziewek: na opończę z sukna cienkiego, hajdawery aksamitne z takimiż łapciami i na świtkę z krajowego samodziału dobornego, którą się w powszednie dni tygodnia obchodził.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(\">\", sent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9J8Jj1Wcpise"
      },
      "source": [
        "Gdy _spaCy_ tworzy dokument, wykorzystuje zasadę _nieniszczącej tokenizacji_, co oznacza, że tokeny, zdania itp. są po prostu indeksami w długiej tablicy. Innymi słowy, nie dzielą strumienia tekstu na małe kawałki. Tak więc każde zdanie to [span](https://spacy.io/api/span) z indeksem _start_ i _end_ w tablicy dokumentów:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBTmYsXKpise"
      },
      "outputs": [],
      "source": [
        "for sent in doc.sents:\n",
        "    print(\">\", sent.start, sent.end)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVZjCZ1spisf"
      },
      "source": [
        "Możemy zaindeksować tablicę dokumentów, aby wyciągnąć tokeny dla jednego zdania:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeoxaT_9pisf"
      },
      "outputs": [],
      "source": [
        "doc[97:129]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_MnOn_xpisw"
      },
      "source": [
        "## Natural Language Understanding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVeDbcvYpisw"
      },
      "outputs": [],
      "source": [
        "text = \"Józef Piłsudski herbu Piłsudski urodził się 5 grudnia 1867 w Zułowie na Wileńszczyźnie, w rodzinie o tradycjach patriotycznych.\"\n",
        "doc = nlp(text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yf8VcaQJpisx"
      },
      "outputs": [],
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_, ent.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlvaqVV6pisx"
      },
      "source": [
        "Biblioteka _displaCy_ zapewnia doskonały sposób wizualizacji nazwanych jednostek:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPJG-UwYpisy"
      },
      "outputs": [],
      "source": [
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wektory"
      ],
      "metadata": {
        "id": "ASKUEn7U8zF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "doc1 = nlp(\"Jedziemy na święta do domu.\")\n",
        "doc2 = nlp(\"Wracamy z wakacji nad morzem.\")\n",
        "doc3 = nlp(\"Uczę się na egzamin.\")\n",
        "\n",
        "print(doc1, \"<->\", doc2, doc1.similarity(doc2))\n",
        "print(doc1, \"<->\", doc3, doc1.similarity(doc3))\n",
        "\n",
        "wakacje = doc1[4]\n",
        "dom = doc2[2]\n",
        "print(wakacje, \"<->\", dom, wakacje.similarity(dom))"
      ],
      "metadata": {
        "id": "eGPH7Kpn82j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zadanie\n",
        "\n",
        "1. Korzystając z API do wikipedii (pierwszy notatnik) pobrać artykuł na dowolny temat i korzystając ze spacy pobrać wszystkie osoby, które w nim występują (korzystając z lematow). Zrobić to analogicznie dla angielskiego (lematyzator do NER działa lepiej po angielsku). Wypisać wszystkie osoby w kolejnosci razem z informacjamą ile razy występują.\n",
        "\n",
        "2. Wybrać dowolną krotką książkę z wolnych lektur. Korzystając z parsowania zależnosciowego wypisać wszystkie przymiotniki okreslające danego bohatera bezpośrednio (dep) albo okreslenie. Analogicznie dla angielskiego, przykład poniżej. Przyjrzeć się strukturze drzewa zależnosciowego i zaproponować ulepszenia - tak, żeby lepiej znajdować interesujące nas okreslenia."
      ],
      "metadata": {
        "id": "lB6n0aL_cnie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://wolnelektury.pl/media/book/txt/kamizelka.txt"
      ],
      "metadata": {
        "id": "NkwKkJBnfYMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re"
      ],
      "metadata": {
        "id": "rqFRT3OQjFyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('kamizelka.txt', 'r') as file:\n",
        "    data = file.read().replace('\\n', '')"
      ],
      "metadata": {
        "id": "dsoLS_Tlf1GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(data)"
      ],
      "metadata": {
        "id": "HGI724iqgs5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "okreslenia = []\n",
        "for sent in doc.sents:\n",
        "    if re.search(\"kamizelk\", str(sent)):\n",
        "      for token in sent:\n",
        "        print(token.text, token.lemma_, token.pos_,  token.dep_, token.head.text,)\n",
        "        if re.search(\"kamizelk\",token.head.text) and (token.dep_ == \"amod\" or token.dep_ == \"nmod\"):\n",
        "          okreslenia.append(token)\n",
        "      print(\"\\n\\n\\n\")"
      ],
      "metadata": {
        "id": "7a8qx_0WikrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(okreslenia)"
      ],
      "metadata": {
        "id": "JFUi_teUi5Bq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}